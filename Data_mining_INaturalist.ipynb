{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "na98opxzi3CS",
        "kfsXl1-PjSZF",
        "Ui9n4yGUnIPM",
        "KNrYcBq75EVy"
      ],
      "authorship_tag": "ABX9TyO4W1wZs4pycrxNaawOt7fD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oleonn/DataMining/blob/main/Data_mining_INaturalist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data mining"
      ],
      "metadata": {
        "id": "j9kC1ibiiiho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup and connection to Google Drive"
      ],
      "metadata": {
        "id": "na98opxzi3CS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXMzNMQsiX4w"
      },
      "outputs": [],
      "source": [
        "!pip install pyinaturalist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyinaturalist.v1.observations import get_observation_species_counts\n",
        "import json\n",
        "from pyinaturalist import (\n",
        "    Observation,\n",
        "    pprint,\n",
        "    get_observations,\n",
        "    get_observation_species_counts\n",
        ")\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 100  #Sert a augmenter la qte de caracteres affiches pour chaque string\n",
        "import random\n",
        "\n",
        "import csv\n",
        "\n",
        "import requests\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "AP5191veiha4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8EY4OMWjAx_",
        "outputId": "5f7b0d0f-164d-420a-be37-05734a8f57e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Directory and parameters"
      ],
      "metadata": {
        "id": "kfsXl1-PjSZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/drive/MyDrive/Projet_mellifere/Donnees/\"\n",
        "sp_classes = [\"Asclepias_syriaca\", \"Daucus_carota\", \"Eutrochium_maculatum\", \"Leucanthemum_vulgare\", \"Solidago_canadensis\"]\n",
        "originals = \"originals\"\n",
        "\n",
        "#Create class folders if they don't already exist in directory\n",
        "for name in sp_classes:\n",
        "  #Create the species folder\n",
        "  sp_classes_path = os.path.join(directory, name)\n",
        "  if not os.path.exists(sp_classes_path):\n",
        "    os.makedirs(sp_classes_path)\n",
        "    print(f\"folder {sp_classes_path} created.\")\n",
        "  #Create the originals subfolder\n",
        "  sp_classes_originals = os.path.join(sp_classes_path, originals)\n",
        "  if not os.path.exists(sp_classes_originals):\n",
        "    os.makedirs(sp_classes_originals)\n",
        "    print(f\"folder {sp_classes_originals} created.\")\n"
      ],
      "metadata": {
        "id": "XO4dJaovkMZi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before executing the next cell, export the occurences from the GBIF website for the targeted species. Make sure to select \"INaturalist (research)\" in the search filters. Download the occurences in Archive Darwin Core format. Open the zip file(s) and extract the \"multimedia.txt\" of every target species into their matching folder (in the species folder that is, not in the \"originals\" one)."
      ],
      "metadata": {
        "id": "deAZE5Dwjmy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data mining"
      ],
      "metadata": {
        "id": "Ui9n4yGUnIPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wished_format = \"small\" #either thumb, small, medium, original or large\"\n",
        "wished_nb = 50 #total number of pictures wished for every species"
      ],
      "metadata": {
        "id": "9OZeu9PkN2Rm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in sp_classes:\n",
        "  #Creation of a random sample (n = wished_nb) from the list of pictures in the \"multimedia.txt\" file\n",
        "  sp_classes_path = os.path.join(directory, name)\n",
        "  img_list_complete = pd.read_csv(sp_classes_path+\"/multimedia.txt\", dtype=str, sep=\"\\t\")[\"identifier\"].tolist()\n",
        "  img_list_used = random.sample(img_list_complete, wished_nb)\n",
        "  for line in range(len(img_list_used)):\n",
        "    img_list_used[line] = img_list_used[line].replace(\"original\", wished_format)\n",
        "\n",
        "  #Import of all sampled pictures\n",
        "  sp_classes_originals = os.path.join(sp_classes_path, originals)\n",
        "  broken_images = []\n",
        "  count = 1\n",
        "  for img in img_list_used:\n",
        "    # We can split the file based on '/' and extract the last split within the Python list below:\n",
        "    file_name = img.split('/')[-2]\n",
        "    file_name = f\"{sp_classes_originals}/{file_name}.jpeg\"  # Update file extension to .jpeg\n",
        "    if count == 1:\n",
        "      print(f\"Download of {wished_nb} files for {name} has started in {sp_classes_originals}\")\n",
        "    if count % 100 == 0 and count < wished_nb:\n",
        "      print(f\"File {count} out of {wished_nb} for {name} has been downloaded in {sp_classes_originals}\")\n",
        "    if count == wished_nb:\n",
        "      print(f\"All {wished_nb} files for {name} have been downloaded in {sp_classes_originals}\")\n",
        "    count = count + 1\n",
        "    # Now let's send a request to the image URL:\n",
        "    r = requests.get(img, stream=True)\n",
        "    # We can check that the status code is 200 before doing anything else:\n",
        "    if r.status_code == 200:\n",
        "        # This command below will allow us to write the data to a file as binary:\n",
        "        with open(file_name, 'wb') as f:\n",
        "            for chunk in r.iter_content(1024):\n",
        "                f.write(chunk)\n",
        "    else:\n",
        "        # We will write all of the images back to the broken_images list:\n",
        "        broken_images.append(img)\n",
        "\n",
        "  with open(f\"{sp_classes_originals}/img_list_used_{name}.csv\", \"w\", newline=\"\") as f:\n",
        "    for item in img_list_used:\n",
        "      f.write(item + \",\\n\")\n",
        "    print(f\"Mission {name} complete.\")\n",
        "\n",
        "print(\"Mission complete. Good work, 007\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDOqj9SIrsav",
        "outputId": "0e95fb35-622e-4388-b975-f08011486838"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download of 50 files for Asclepias_syriaca has started in /content/drive/MyDrive/Projet_mellifere/Donnees/Asclepias_syriaca/originals\n",
            "All 50 files for Asclepias_syriaca have been downloaded in /content/drive/MyDrive/Projet_mellifere/Donnees/Asclepias_syriaca/originals\n",
            "Mission Asclepias_syriaca complete.\n",
            "Download of 50 files for Daucus_carota has started in /content/drive/MyDrive/Projet_mellifere/Donnees/Daucus_carota/originals\n",
            "All 50 files for Daucus_carota have been downloaded in /content/drive/MyDrive/Projet_mellifere/Donnees/Daucus_carota/originals\n",
            "Mission Daucus_carota complete.\n",
            "Download of 50 files for Eutrochium_maculatum has started in /content/drive/MyDrive/Projet_mellifere/Donnees/Eutrochium_maculatum/originals\n",
            "All 50 files for Eutrochium_maculatum have been downloaded in /content/drive/MyDrive/Projet_mellifere/Donnees/Eutrochium_maculatum/originals\n",
            "Mission Eutrochium_maculatum complete.\n",
            "Download of 50 files for Leucanthemum_vulgare has started in /content/drive/MyDrive/Projet_mellifere/Donnees/Leucanthemum_vulgare/originals\n",
            "All 50 files for Leucanthemum_vulgare have been downloaded in /content/drive/MyDrive/Projet_mellifere/Donnees/Leucanthemum_vulgare/originals\n",
            "Mission Leucanthemum_vulgare complete.\n",
            "Download of 50 files for Solidago_canadensis has started in /content/drive/MyDrive/Projet_mellifere/Donnees/Solidago_canadensis/originals\n",
            "All 50 files for Solidago_canadensis have been downloaded in /content/drive/MyDrive/Projet_mellifere/Donnees/Solidago_canadensis/originals\n",
            "Mission Solidago_canadensis complete.\n",
            "Mission complete. Good work, 007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image size validation"
      ],
      "metadata": {
        "id": "KNrYcBq75EVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if every image has at least 128 pixels of width and height, and deletion of those that don't\n",
        "def size_check():\n",
        "  for name in sp_classes:\n",
        "    sp_classes_path = os.path.join(directory, name)\n",
        "    sp_classes_originals = os.path.join(sp_classes_path, originals)\n",
        "\n",
        "    for filename in os.listdir(sp_classes_originals):\n",
        "      if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "        img_path = os.path.join(sp_classes_originals, filename)\n",
        "        img = Image.open(img_path)\n",
        "        width, height = img.size\n",
        "\n",
        "        if width < 128 or height < 128:\n",
        "          print(f\"{img_path} is too small (w {width} x h {height}) and has been deleted\")\n",
        "          os.remove(img_path)\n",
        "\n",
        "    print(f\"{name} checked with no undersized images to declare\")\n",
        "  print(\"Mission complete! 10-4\")\n",
        "\n",
        "\n",
        "#Checking if images have been deleted, and download of additional pictures if it's the case\n",
        "def redownload():\n",
        "  for name in sp_classes:\n",
        "    sp_classes_path = os.path.join(directory, name)\n",
        "    sp_classes_originals = os.path.join(sp_classes_path, originals)\n",
        "\n",
        "    img_list = []\n",
        "    for filename in os.listdir(sp_classes_originals):\n",
        "      if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "        img_list.append(filename)\n",
        "    print(f\"Number of images for {name} : {len(img_list)} out of {wished_nb} expected\")\n",
        "\n",
        "    if len(img_list) < wished_nb: #If a redownload is necessary\n",
        "      compensation = wished_nb - len(img_list)\n",
        "      img_list_used_csv = f\"{sp_classes_originals}/img_list_used_{name}.csv\"\n",
        "      #Reading the CSV file\n",
        "      with open(img_list_used_csv, \"r\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        used_items = [row[0] for row in reader]\n",
        "        for line in range(len(used_items)): #Replace the wished format for the \"original\" format in the links of the used_items list. Makes comparison with the img_list_complete possible\n",
        "          used_items[line] = used_items[line].replace(wished_format, \"original\")\n",
        "\n",
        "      #Creation of a random sample (n = compensation) from the list of pictures (only the unused) in the \"multimedia.txt\" file\n",
        "      img_list_complete = pd.read_csv(sp_classes_path+\"/multimedia.txt\", dtype=str, sep=\"\\t\")[\"identifier\"].tolist()\n",
        "      unused_items = [item for item in img_list_complete if item not in used_items] #Finding unused items\n",
        "      #Additional selection\n",
        "      new_items = random.sample(unused_items, compensation)\n",
        "      for line in range(len(new_items)): #Replace the wished format for the \"original\" format in the links of the used_items list. Makes comparison with the img_list_complete possible\n",
        "        new_items[line] = new_items[line].replace(\"original\", wished_format)\n",
        "      #Appending to CSV (updating the .csv file to inclued newly selected items)\n",
        "      with open(img_list_used_csv, \"a\", newline=\"\") as f:\n",
        "        for item in new_items:\n",
        "          f.write(item + \",\\n\")\n",
        "\n",
        "      #Downloading the newly sampled images\n",
        "      broken_images = []\n",
        "      count = 1\n",
        "      for img in new_items:\n",
        "        # We can split the file based on '/' and extract the last split within the Python list below:\n",
        "        file_name = img.split('/')[-2]\n",
        "        file_name = f\"{sp_classes_originals}/{file_name}.jpeg\"  # Update file extension to .jpeg\n",
        "        if count == 1:\n",
        "          print(f\"Download of {compensation} files for {name} has started in {sp_classes_originals}\")\n",
        "        if count % 10 == 0 and count < compensation:\n",
        "          print(f\"File {count} out of {compensation} for {name} has been downloaded in {sp_classes_originals}\")\n",
        "        if count == compensation:\n",
        "          print(f\"All {compensation} additional files for {name} have been downloaded in {sp_classes_originals}\")\n",
        "        count = count + 1\n",
        "        # Now let's send a request to the image URL:\n",
        "        r = requests.get(img, stream=True)\n",
        "        # We can check that the status code is 200 before doing anything else:\n",
        "        if r.status_code == 200:\n",
        "            # This command below will allow us to write the data to a file as binary:\n",
        "            with open(file_name, 'wb') as f:\n",
        "                for chunk in r.iter_content(1024):\n",
        "                    f.write(chunk)\n",
        "        else:\n",
        "            # We will write all of the images back to the broken_images list:\n",
        "            broken_images.append(img)"
      ],
      "metadata": {
        "id": "Y4CxABYI5MLv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternate between the two following cells to\n",
        "1) check the size of every picture and delete those that are under 128pixels of width and height\n",
        "2) redownload pictures to replace the deleted ones with new, newly sampled and so far unused pictures\n",
        "Make sure the Google Drive has had time to synch (it may take up to 10 seconds) after each redownload"
      ],
      "metadata": {
        "id": "Xx52d371Ql6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcm5jiktFe25",
        "outputId": "6c2c7c4a-0896-4ce7-f769-05d296741a0e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asclepias_syriaca checked with no undersized images to declare\n",
            "Daucus_carota checked with no undersized images to declare\n",
            "Eutrochium_maculatum checked with no undersized images to declare\n",
            "Leucanthemum_vulgare checked with no undersized images to declare\n",
            "Solidago_canadensis checked with no undersized images to declare\n",
            "Mission complete! 10-4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "redownload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpNXYE52LGwg",
        "outputId": "42cdecde-1e5d-4599-e206-139a29b17d27"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images for Asclepias_syriaca : 50 out of 50 expected\n",
            "Number of images for Daucus_carota : 50 out of 50 expected\n",
            "Number of images for Eutrochium_maculatum : 50 out of 50 expected\n",
            "Number of images for Leucanthemum_vulgare : 50 out of 50 expected\n",
            "Number of images for Solidago_canadensis : 50 out of 50 expected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cropping"
      ],
      "metadata": {
        "id": "CWP1_6yfRON2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cropping as a centered 128px square"
      ],
      "metadata": {
        "id": "9MEVhzSIIO4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "center_cropped = \"center_cropped\"\n",
        "\n",
        "for name in sp_classes:\n",
        "  #Create the center cropped subfolder\n",
        "  sp_classes_path = os.path.join(directory, name)\n",
        "  sp_classes_originals = os.path.join(sp_classes_path, originals)\n",
        "  sp_classes_cropped = os.path.join(sp_classes_path, center_cropped)\n",
        "  if not os.path.exists(sp_classes_cropped):\n",
        "      os.makedirs(sp_classes_cropped)\n",
        "      print(f\"folder {sp_classes_cropped} created.\")\n",
        "\n",
        "  #Center crop\n",
        "  print(f\"Cropping of {name} images has started. Data will be saved in {sp_classes_cropped}\")\n",
        "  for filename in os.listdir(sp_classes_originals):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "      img_path = os.path.join(sp_classes_originals, filename)\n",
        "      img = Image.open(img_path)\n",
        "      width, height = img.size\n",
        "      #Setting where to crop\n",
        "      left = (width-128)/2 + ((width-128)/2)%1 #The modulo section here make it so that the answer is always a whole number\n",
        "      right = left +128\n",
        "      top = (height-128)/2 + ((height-128)/2)%1\n",
        "      bottom = top +128\n",
        "      #Crop and save in center_cropped folder\n",
        "      img_cropped = img.crop((left,top,right,bottom))\n",
        "      file_name_cropped = f\"{sp_classes_cropped}/{filename}\"\n",
        "      img_cropped.save(file_name_cropped)\n",
        "  print(f\"{name} images have been cropped to 128x128px and saved in {sp_classes_cropped}\")"
      ],
      "metadata": {
        "id": "FLxj2ZXbRRLh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}