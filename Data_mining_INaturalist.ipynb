{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "na98opxzi3CS"
      ],
      "authorship_tag": "ABX9TyPz47hBkbbIT1TEIGh3Ii14",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oleonn/DataMining/blob/main/Data_mining_INaturalist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data mining"
      ],
      "metadata": {
        "id": "j9kC1ibiiiho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup and connection to Google Drive"
      ],
      "metadata": {
        "id": "na98opxzi3CS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXMzNMQsiX4w",
        "outputId": "39dedb4f-deae-4fe6-de28-65596ca8cbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyinaturalist\n",
            "  Downloading pyinaturalist-0.19.0-py3-none-any.whl (143 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m102.4/143.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from pyinaturalist) (23.2.0)\n",
            "Requirement already satisfied: keyring>=22.3 in /usr/lib/python3/dist-packages (from pyinaturalist) (23.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=3.0 in /usr/local/lib/python3.10/dist-packages (from pyinaturalist) (3.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.6 in /usr/local/lib/python3.10/dist-packages (from pyinaturalist) (4.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.0 in /usr/local/lib/python3.10/dist-packages (from pyinaturalist) (2.8.2)\n",
            "Collecting python-forge>=18.6 (from pyinaturalist)\n",
            "  Downloading python_forge-18.6.0-py35-none-any.whl (31 kB)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.10/dist-packages (from pyinaturalist) (2.31.0)\n",
            "Collecting requests-cache>=1.1 (from pyinaturalist)\n",
            "  Downloading requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-ratelimiter>=0.3.2 (from pyinaturalist)\n",
            "  Downloading requests_ratelimiter-0.6.0-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: rich>=10.9 in /usr/local/lib/python3.10/dist-packages (from pyinaturalist) (13.7.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=3.0->pyinaturalist) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.0->pyinaturalist) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->pyinaturalist) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->pyinaturalist) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->pyinaturalist) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->pyinaturalist) (2024.6.2)\n",
            "Collecting cattrs>=22.2 (from requests-cache>=1.1->pyinaturalist)\n",
            "  Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting url-normalize>=1.4 (from requests-cache>=1.1->pyinaturalist)\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting pyrate-limiter<3.0 (from requests-ratelimiter>=0.3.2->pyinaturalist)\n",
            "  Downloading pyrate_limiter-2.10.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.9->pyinaturalist) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache>=1.1->pyinaturalist) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions!=4.6.3,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache>=1.1->pyinaturalist) (4.12.2)\n",
            "Installing collected packages: python-forge, url-normalize, pyrate-limiter, cattrs, requests-ratelimiter, requests-cache, pyinaturalist\n",
            "Successfully installed cattrs-23.2.3 pyinaturalist-0.19.0 pyrate-limiter-2.10.0 python-forge-18.6.0 requests-cache-1.2.1 requests-ratelimiter-0.6.0 url-normalize-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyinaturalist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyinaturalist.v1.observations import get_observation_species_counts\n",
        "import json\n",
        "from pyinaturalist import (\n",
        "    Observation,\n",
        "    pprint,\n",
        "    get_observations,\n",
        "    get_observation_species_counts\n",
        ")\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 100  #Sert a augmenter la qte de caracteres affiches pour chaque string\n",
        "import random\n",
        "\n",
        "import csv\n",
        "\n",
        "import requests\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "AP5191veiha4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8EY4OMWjAx_",
        "outputId": "f94b7f8f-51ba-48de-d9f3-1cc0a10ad866"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Directory and parameters"
      ],
      "metadata": {
        "id": "kfsXl1-PjSZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/drive/MyDrive/Projet_mellifere/Donnees/\"\n",
        "sp_classes = [\"Asclepias_syriaca\", \"Daucus_carota\", \"Eutrochium_maculatum\", \"Leucanthemum_vulgare\", \"Solidago_canadensis\"]\n",
        "originals = \"originals\"\n",
        "\n",
        "#Create class folders if they don't already exist in directory\n",
        "for name in sp_classes:\n",
        "  #Create the species folder\n",
        "  sp_classes_path = os.path.join(directory, name)\n",
        "  if not os.path.exists(sp_classes_path):\n",
        "    os.makedirs(sp_classes_path)\n",
        "    print(f\"folder {sp_classes_path} created.\")\n",
        "  #Create the originals subfolder\n",
        "  sp_classes_originals = os.path.join(sp_classes_path, originals)\n",
        "  if not os.path.exists(sp_classes_originals):\n",
        "    os.makedirs(sp_classes_originals)\n",
        "    print(f\"folder {sp_classes_originals} created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO4dJaovkMZi",
        "outputId": "61973a8a-c6fc-471c-ca27-5bf0a0b818c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "folder /content/drive/MyDrive/Projet_mellifere/Donnees/Asclepias_syriaca/originals created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before executing the next cell, export the occurences from the GBIF website for the targeted species. Make sure to select \"INaturalist (research)\" in the search filters. Download the occurences in Archive Darwin Core format. Open the zip file(s) and extract the \"multimedia.txt\" of every target species into their matching folder (in the species folder that is, not in the \"originals\" one)."
      ],
      "metadata": {
        "id": "deAZE5Dwjmy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data mining"
      ],
      "metadata": {
        "id": "Ui9n4yGUnIPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wished_format = \"small\" #either thumb, small, medium, original or large\"\n",
        "wished_nb = 2000\n",
        "originals = \"originals\"\n",
        "\n",
        "for name in sp_classes:\n",
        "  sp_classes_path = os.path.join(directory, name)\n",
        "  img_list_complete = pd.read_csv(sp_classes_path+\"/multimedia.txt\", dtype=str, sep=\"\\t\")[\"identifier\"].tolist()\n",
        "  img_list_used = random.sample(img_list_complete, wished_nb)\n",
        "  for line in range(len(img_list_used)):\n",
        "    img_list_used[line] = img_list_used[line].replace(\"original\", wished_format)\n",
        "\n",
        "  sp_classes_originals = os.path.join(sp_classes_path, originals)\n",
        "  broken_images = []\n",
        "  count = 1\n",
        "  for img in img_list_used:\n",
        "    # We can split the file based on '/' and extract the last split within the Python list below:\n",
        "    file_name = img.split('/')[-2]\n",
        "    file_name = f\"{sp_classes_originals}/{file_name}.jpeg\"  # Update file extension to .jpeg\n",
        "    if count == 1:\n",
        "      print(f\"Download of {wished_nb} files for {name} has started in {sp_classes_originals}\")\n",
        "    if count % 100 == 0 and count < wished_nb:\n",
        "      print(f\"File {count} out of {wished_nb} for {name} has been downloaded in {sp_classes_originals}\")\n",
        "    if count == wished_nb:\n",
        "      print(f\"All {wished_nb} files for {name} have been downloaded in {sp_classes_originals}\")\n",
        "    count = count + 1\n",
        "    # Now let's send a request to the image URL:\n",
        "    r = requests.get(img, stream=True)\n",
        "    # We can check that the status code is 200 before doing anything else:\n",
        "    if r.status_code == 200:\n",
        "        # This command below will allow us to write the data to a file as binary:\n",
        "        with open(file_name, 'wb') as f:\n",
        "            for chunk in r.iter_content(1024):\n",
        "                f.write(chunk)\n",
        "    else:\n",
        "        # We will write all of the images back to the broken_images list:\n",
        "        broken_images.append(img)\n",
        "\n",
        "  with open(f\"{sp_classes_originals}/img_list_used_{name}.csv\", \"w\", newline=\"\") as f:\n",
        "    for item in img_list_used:\n",
        "      f.write(item + \",\\n\")\n",
        "    print(f\"Mission {name} complete.\")\n",
        "\n",
        "print(\"Mission complete. Good work, 007\")"
      ],
      "metadata": {
        "id": "QDOqj9SIrsav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}